{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Zgh5uzNtYG7"
      },
      "outputs": [],
      "source": [
        "# spark_cleaning\n",
        "# -----------------------------------------------\n",
        "# Yelp Dataset Cleaning & Preparation using PySpark\n",
        "# -----------------------------------------------\n",
        "\n",
        "from pyspark.sql.functions import col, length, to_timestamp\n",
        "\n",
        "# -------------------------------\n",
        "# Load Raw Yelp Datasets from GCS\n",
        "# -------------------------------\n",
        "business_df = spark.read.json(\"gs://yelpbigdata/business.json\")\n",
        "review_df = spark.read.json(\"gs://yelpbigdata/review.json\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 1: Preprocessing - Column Renaming\n",
        "# ------------------------------------------\n",
        "# Rename ambiguous column to avoid collision\n",
        "review_renamed_df = review_df.withColumnRenamed(\"stars\", \"review_stars\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 2: Join Datasets on business_id\n",
        "# ------------------------------------------\n",
        "joined_df = review_renamed_df.join(business_df, on=\"business_id\", how=\"inner\")\n",
        "\n",
        "# Drop 'stars' from business_df to avoid confusion\n",
        "cleaned_joined_df = joined_df.drop(business_df.stars)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 3: Basic Cleaning\n",
        "# ------------------------------------------\n",
        "# Remove rows with nulls in essential columns\n",
        "cleaned_joined_df = cleaned_joined_df.dropna(\n",
        "    subset=[\"business_id\", \"text\", \"review_stars\", \"date\", \"city\"]\n",
        ")\n",
        "\n",
        "# Cast date to proper timestamp\n",
        "cleaned_joined_df = cleaned_joined_df.withColumn(\"date\", to_timestamp(col(\"date\")))\n",
        "\n",
        "# Filter out very short reviews (text length < 50)\n",
        "cleaned_joined_df = cleaned_joined_df.filter(length(col(\"text\")) >= 50)\n",
        "\n",
        "# Filter for valid rating values (1 to 5)\n",
        "cleaned_joined_df = cleaned_joined_df.filter(\n",
        "    (col(\"review_stars\") >= 1) & (col(\"review_stars\") <= 5)\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 4: Business Category Filtering\n",
        "# ------------------------------------------\n",
        "# Only keep restaurants\n",
        "cleaned_joined_df = cleaned_joined_df.filter(col(\"categories\").like(\"%Restaurants%\"))\n",
        "\n",
        "# Keep only rows with known price range and WiFi info\n",
        "cleaned_joined_df = cleaned_joined_df.filter(\n",
        "    col(\"attributes.RestaurantsPriceRange2\").isNotNull() &\n",
        "    col(\"attributes.WiFi\").isNotNull()\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 5: Select Final Columns for Analysis\n",
        "# ------------------------------------------\n",
        "final_df = cleaned_joined_df.select(\n",
        "    \"business_id\", \"text\", \"review_stars\", \"date\", \"city\", \"state\",\n",
        "    \"categories\", \"name\", \"review_count\",\n",
        "    col(\"attributes.RestaurantsPriceRange2\").alias(\"RestaurantsPriceRange2\"),\n",
        "    col(\"attributes.WiFi\").alias(\"WiFi\")\n",
        ")\n",
        "\n",
        "# Preview sample output and statistics\n",
        "final_df.show(5, truncate=False)\n",
        "final_df.describe([\"review_stars\"]).show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 6: Save Cleaned Data as Parquet to GCS\n",
        "# ------------------------------------------\n",
        "final_df.write.mode(\"overwrite\").parquet(\"gs://yelpbigdata/final_df.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spark_analysis\n",
        "# -----------------------------------------------\n",
        "# Sentiment Aggregation & Exploratory Analysis using PySpark\n",
        "# -----------------------------------------------\n",
        "\n",
        "from pyspark.sql.functions import when, col, avg, count\n",
        "import time\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Step 1: Add Sentiment Column Based on Rating\n",
        "# -----------------------------------------------\n",
        "# Define sentiment label from review_stars (1–5)\n",
        "# ≤ 2 => Negative, 3 => Neutral, ≥ 4 => Positive\n",
        "sentiment_df = final_df.withColumn(\n",
        "    \"sentiment\",\n",
        "    when(col(\"review_stars\") <= 2, \"negative\")\n",
        "    .when(col(\"review_stars\") == 3, \"neutral\")\n",
        "    .otherwise(\"positive\")\n",
        ")\n",
        "\n",
        "# Create a temporary SQL view\n",
        "sentiment_df.createOrReplaceTempView(\"sentiment_reviews\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Step 2: Sentiment Distribution Query\n",
        "# -----------------------------------------------\n",
        "start_time = time.time()\n",
        "\n",
        "sentiment_counts = spark.sql(\"\"\"\n",
        "    SELECT sentiment, COUNT(*) AS total_reviews\n",
        "    FROM sentiment_reviews\n",
        "    GROUP BY sentiment\n",
        "    ORDER BY total_reviews DESC\n",
        "\"\"\")\n",
        "\n",
        "sentiment_counts.show()\n",
        "print(f\"Sentiment count query execution time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Step 3: Average Review Stars per City\n",
        "# -----------------------------------------------\n",
        "start_time = time.time()\n",
        "\n",
        "avg_stars_by_city = final_df.groupBy(\"city\").agg(\n",
        "    avg(\"review_stars\").alias(\"avg_review_stars\"),\n",
        "    count(\"*\").alias(\"num_reviews\")\n",
        ").orderBy(col(\"avg_review_stars\").desc())\n",
        "\n",
        "avg_stars_by_city.show(20)\n",
        "print(f\"City-level analysis execution time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Step 4: Review Count Distribution per Business\n",
        "# -----------------------------------------------\n",
        "start_time = time.time()\n",
        "\n",
        "review_count_distribution = final_df.groupBy(\"review_count\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"review_count\")\n",
        "\n",
        "review_count_distribution.show(20)\n",
        "print(f\"Review count distribution execution time: {time.time() - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "Pv4BwdNVtvRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# machine-learning\n",
        "# -------------------------\n",
        "# Step 1: Import Libraries\n",
        "# -------------------------\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Create Sentiment Label\n",
        "# -------------------------\n",
        "df_labeled = final_df.withColumn(\n",
        "    \"sentiment\",\n",
        "    when(final_df[\"review_stars\"] >= 4, \"positive\")\n",
        "    .when(final_df[\"review_stars\"] == 3, \"neutral\")\n",
        "    .otherwise(\"negative\")\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Step 3: Filter required columns and drop nulls\n",
        "# -------------------------\n",
        "df_model = df_labeled.select(\"text\", \"sentiment\").dropna()\n",
        "\n",
        "# -------------------------\n",
        "# Step 4: Define Pipeline Stages\n",
        "# -------------------------\n",
        "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=5000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=30, maxDepth=7, maxBins=32)\n",
        "\n",
        "# -------------------------\n",
        "# Step 5: Build Pipeline\n",
        "# -------------------------\n",
        "pipeline = Pipeline(stages=[indexer, tokenizer, remover, hashingTF, idf, rf])\n",
        "\n",
        "# -------------------------\n",
        "# Step 6: Train-Test Split\n",
        "# -------------------------\n",
        "train_data, test_data = df_model.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# -------------------------\n",
        "# Step 7: Fit Model\n",
        "# -------------------------\n",
        "rf_model = pipeline.fit(train_data)\n",
        "\n",
        "# -------------------------\n",
        "# Step 8: Evaluate Performance\n",
        "# -------------------------\n",
        "predictions = rf_model.transform(test_data)\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "\n",
        "print(f\"F1 Score: {f1_score:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Step 9: Feature Importances\n",
        "# -------------------------\n",
        "rf_stage = rf_model.stages[-1]  # Extract RandomForestClassifier from pipeline\n",
        "importances = rf_stage.featureImportances\n",
        "importance_list = sorted([(idx, importance) for idx, importance in enumerate(importances)], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Save to file\n",
        "with open(\"/tmp/model_metrics_and_importance.txt\", \"w\") as f:\n",
        "    f.write(\"Top 20 Feature Importances (by index):\\n\")\n",
        "    for idx, score in importance_list[:20]:\n",
        "        f.write(f\"Feature {idx}: Importance {score:.6f}\\n\")\n",
        "\n",
        "# Upload to GCS\n",
        "!gsutil cp /tmp/model_metrics_and_importance.txt gs://yelpbigdata/results\n",
        "\n",
        "# -------------------------\n",
        "# Step 10: Confusion Matrix\n",
        "# -------------------------\n",
        "preds_and_labels = predictions.select(\"prediction\", \"label\").toPandas()\n",
        "cm = confusion_matrix(preds_and_labels[\"label\"], preds_and_labels[\"prediction\"])\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.savefig(\"/tmp/confusion_matrix.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "!gsutil cp /tmp/confusion_matrix.png gs://yelpbigdata/images"
      ],
      "metadata": {
        "id": "gwTMwJP8u9Dx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}